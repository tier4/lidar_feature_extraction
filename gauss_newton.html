
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>姿勢最適化 &#8212; Localization  documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="外れ値に頑強な標準偏差推定法" href="robust_stddev_estimator.html" />
    <link rel="prev" title="Welcome to Localization’s documentation!" href="index.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="id1">
<h1>姿勢最適化<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h1>
<section id="gauss-newton">
<h2>重み付きGauss-Newton<a class="headerlink" href="#gauss-newton" title="Permalink to this heading">¶</a></h2>
<p>Gauss-Newton法はその収束の速さからさまざまな場面で扱われる数理最適化手法であるが、一般的に誤差関数として二乗誤差を用いるため、外れ値に弱いという問題がある。</p>
<p>ここでは外れ値に対して頑強な誤差関数とその最適化手法について解説する。例として point-set registration を取り扱うが、ここで解説する手法は他のさまざまな最適化問題にも適用することができる。</p>
<section id="id2">
<h3>問題設定<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h3>
<p>最適化問題のパラメータを <span class="math notranslate nohighlight">\(\mathbf{\beta} = \{\mathbf{q}, \mathbf{t}\}\)</span> で表現する。ここで <span class="math notranslate nohighlight">\(\mathbf{q} \in \mathbb{H}\)</span> は回転を表す四元数、 <span class="math notranslate nohighlight">\(\mathbf{t} \in \mathbb{R}^{3}\)</span> は並進を表す3次元ベクトルである。</p>
<p>2つの3次元点群の集合をそれぞれ <span class="math notranslate nohighlight">\(P\)</span> と <span class="math notranslate nohighlight">\(U\)</span> とする。</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
P &amp;= \{\mathbf{p}_{i}\},\,\mathbf{p}_{i} \in \mathrm{R}^{3},\,i=1,...,n \\
U &amp;= \{\mathbf{u}_{j}\},\,\mathbf{u}_{j} \in \mathrm{R}^{3},\,j=1,...,n
\end{align}\end{split}\]</div>
<p>Point-set registration とは、これら点群を用いてなんらかの誤差関数 <span class="math notranslate nohighlight">\(E\)</span> を設定し、それを最小化することで、これら点群の間の尤もらしい変換 <span class="math notranslate nohighlight">\(\mathbf{\beta} = \{\mathbf{q}, \mathbf{t}\}\)</span> を求める問題である。</p>
<div class="math notranslate nohighlight">
\[\underset{\mathbf{\beta}}{\arg\min}\, E(P, U;\, \mathbf{\beta}),\]</div>
<p>誤差関数には一般的に平均二乗誤差がよく用いられる。</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\mathrm{MSE}(P, U; \mathbf{\beta}) &amp;= \frac{1}{n} \sum_{i=1}^{n} || \mathbf{r}(\mathbf{p}_{i}, \mathbf{u}_{i};\, \mathbf{\beta}) ||^{2} \\[10pt]
&amp;\text{where}\quad\mathbf{r}(\mathbf{p}_{i}, \mathbf{u}_{i};\, \mathbf{\beta}) = R(\mathbf{q}) \cdot \mathbf{p}_{i} + \mathbf{t} - \mathbf{u}_{i}
\end{align}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\mathbf{p}_{i}\)</span> と <span class="math notranslate nohighlight">\(\mathbf{u}_{i}\)</span> はパラメータ <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> の変化の影響を受けない値であり、 <span class="math notranslate nohighlight">\(n\)</span> も最適化のプロセス内では固定値として扱えるので、簡単のために誤差関数を <span class="math notranslate nohighlight">\(E_{s}\)</span> としてこう表記しておこう。</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
E_{s}(\mathbf{\beta}) &amp;= \sum_{i=1}^{n} e_{i}(\mathbf{\beta}) \\[10pt]
e_{i}(\mathbf{\beta}) &amp;= || \mathbf{r}_{i}(\mathbf{\beta}) ||^{2} = \mathbf{r}_{i}(\mathbf{\beta})^{\top}\mathbf{r}_{i}(\mathbf{\beta}) \\[10pt]
\mathbf{r}_{i}(\mathbf{\beta}) &amp;= R(\mathbf{q}) \cdot \mathbf{p}_{i} + \mathbf{t} - \mathbf{u}_{i}
\end{align}\end{split}\]</div>
</section>
<section id="id3">
<h3>外れ値に対する脆弱性<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h3>
<p>さて、ここで問題がある。平均二乗誤差は外れ値に弱いため、入力データ <span class="math notranslate nohighlight">\(P,\, U\)</span> に少しでも外れ値が含まれていると、パラメータの推定結果 <span class="math notranslate nohighlight">\(\hat{\mathbf{\beta}}\)</span> は真の値 <span class="math notranslate nohighlight">\(\mathbf{\beta}^{*}\)</span> から大きく外れてしまう。</p>
</section>
<section id="id4">
<h3>外れ値に対する頑強性の確保<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h3>
<p>この問題に対処するための方法は数多く提案されており、ここではGauss-Newtonの最適化プロセスとともによく用いられる重み付けGauss-Newton法を紹介する。
これはある関数 <span class="math notranslate nohighlight">\(\rho\)</span> を導入することで、平均二乗誤差よりも外れ値の影響を受けにくくする手法である。</p>
<div class="math notranslate nohighlight">
\[E_{\rho}(\mathbf{\beta}) = \sum_{i=1}^{n} \rho(\tilde{e}_{i}(\mathbf{\beta})),\,\quad\tilde{e}_{i}(\mathbf{\beta}) = \frac{e_{i}(\mathbf{\beta})}{\sigma_{MAD}}\]</div>
<p>関数 <span class="math notranslate nohighlight">\(\rho\)</span> はしばしば入力値の標準偏差が1に正規化されていることを仮定する。 <span class="math notranslate nohighlight">\(\sigma_{MAD}\)</span> は <span class="math notranslate nohighlight">\(e_{i}\)</span> の標準偏差の推定値であり、これで正規化を行っている。
また、データに外れ値が含まれていることを仮定するため、標準偏差の計算には外れ値に対して頑強な計算手法が用いられる。詳しくは別項で解説する。</p>
<div class="math notranslate nohighlight">
\[\sigma_{MAD}=\frac{\operatorname{MAD}}{\Phi^{-1}(\frac{3}{4})},\quad\operatorname{MAD}=\operatorname{median}_{i}(\left|e_{i}−\operatorname{median}_{j}(e_{j})\right|)\]</div>
<p><span class="math notranslate nohighlight">\(\rho\)</span> にはさまざまなものが提案されており、たとえば huber loss</p>
<div class="math notranslate nohighlight">
\[\begin{split}\rho(e) = \begin{cases}
    e,          &amp; \text{if } e\lt k^2\\
    2k\sqrt{e} - k^2,  &amp; \text{if } e\geq k^2\\
\end{cases}\end{split}\]</div>
<p>などがよく用いられる。</p>
<figure class="align-default" id="id6">
<a class="reference internal image-reference" href="_images/square_vs_huber.png"><img alt="_images/square_vs_huber.png" src="_images/square_vs_huber.png" style="width: 640.0px; height: 480.0px;" /></a>
<figcaption>
<p><span class="caption-text">通常の二乗誤差とHuber関数の比較(k=2.0)。二乗誤差と比較すると、Huber関数は大きな残差を持つサンプルに対して小さな誤差を割り当てるため、外れ値の影響が小さくなる。</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>通常の二乗誤差は大きな残差 <span class="math notranslate nohighlight">\(\mathbf{r}_{i}\)</span> を持つサンプル(外れ値)に対して大きな誤差 <span class="math notranslate nohighlight">\(e_{i}\)</span> を割り当てる。最適なパラメータ <span class="math notranslate nohighlight">\({\arg\min}_{\mathbf{\beta}} E_{s}\)</span> を求める際には、外れ値に対応する誤差を重点的に減少させようとしてしまい、結果として推定値 <span class="math notranslate nohighlight">\(\hat{\mathbf{\beta}}\)</span> が外れ値に引っ張られてしまう。一方でHuber関数などは外れ値が生み出す誤差を <span class="math notranslate nohighlight">\(\rho(e_{i})\)</span> として抑制するため、外れ値が生み出す誤差を重点的に最小化しようとせず、結果としてロバストな推定ができるようになる。</p>
</section>
<section id="id5">
<h3>Gauss-Newton法による誤差最小化<a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h3>
<p>さて、外れ値に対する頑強性を確保する関数 <span class="math notranslate nohighlight">\(\rho\)</span> を導入したので、これ考慮しつつ誤差 <span class="math notranslate nohighlight">\(E_{\rho}(\mathbf{\beta})\)</span> を最小化する方法を導出しよう。</p>
<p>通常のGauss-Newton法と同じ枠組みで誤差最小化を行う。<span class="math notranslate nohighlight">\(\mathbf{\beta}_{0}\)</span> は初期値として固定されているため、 <span class="math notranslate nohighlight">\(\mathbf{\delta}\)</span> のみを変動させ、誤差の値の変化を観察すればよい。
ある値 <span class="math notranslate nohighlight">\(\mathbf{\beta}_0\)</span> の周辺で関数 <span class="math notranslate nohighlight">\(E_{\rho}\)</span> を近似し、これを最小化するパラメータ <span class="math notranslate nohighlight">\(\mathbf{\beta}_0 + \mathbf{\delta}^{*},\,\mathbf{\delta}^{*} = {\arg\min}_{\mathbf{\delta}}\, E_{\rho}(\mathbf{\beta}_0 + \mathbf{\delta})\)</span> を求めよう。</p>
<p>微小な変数 <span class="math notranslate nohighlight">\(\Delta \mathbf{\delta}\)</span> を導入し、 <span class="math notranslate nohighlight">\(E_{\rho}\)</span> を微分してその変化を観察することで、<span class="math notranslate nohighlight">\(E_{\rho}\)</span> を <span class="math notranslate nohighlight">\(\mathbf{\beta}_{0}\)</span> の周辺で局所的に最小化するパラメータ <span class="math notranslate nohighlight">\(\mathbf{\beta}_{0} + \mathbf{\delta}\)</span> を見つけることができる。</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\frac{\partial E_{\rho}(\mathbf{\beta})}{\partial \mathbf{\beta}}\Big|_{\mathbf{\beta}_{0} + \mathbf{\delta}}
&amp;=
\lim_{\Delta\mathbf{\delta} \to \mathbf{0}}
\frac{E_{\rho}(\mathbf{\beta}_{0} + \mathbf{\delta} + \Delta\mathbf{\delta}) - E_{\rho}(\mathbf{\beta}_{0} + \mathbf{\delta})}
{(\mathbf{\beta}_{0} + \mathbf{\delta} + \Delta\mathbf{\delta}) - (\mathbf{\beta}_{0} + \mathbf{\delta})} \\
&amp;=
\lim_{\Delta\mathbf{\delta} \to \mathbf{0}}
\sum_{i=1}^{n}
\left[
\frac
{\rho(\tilde{e}_{i}(\mathbf{\beta}_{0} + \mathbf{\delta} + \Delta\mathbf{\delta})) - \rho(\tilde{e}_{i}(\mathbf{\beta}_{0} + \mathbf{\delta}))}
{\tilde{e}_{i}(\mathbf{\beta}_{0} + \mathbf{\delta} + \Delta\mathbf{\delta}) - \tilde{e}_{i}(\mathbf{\beta}_{0} + \mathbf{\delta})}
\cdot
\frac
{\tilde{e}_{i}(\mathbf{\beta}_{0} + \mathbf{\delta} + \Delta\mathbf{\delta}) - \tilde{e}_{i}(\mathbf{\beta}_{0} + \mathbf{\delta})}
{(\mathbf{\beta}_{0} + \mathbf{\delta} + \Delta\mathbf{\delta}) - (\mathbf{\beta}_{0} + \mathbf{\delta})}
\right] \\
&amp;=
\lim_{\Delta\mathbf{\delta} \to \mathbf{0}}
\sum_{i=1}^{n}
\left[
\frac
{\partial \rho}{\partial \tilde{e}_{i}}\Big|_{\tilde{e}_{i}(\mathbf{\beta}_{0} + \mathbf{\delta})}
\cdot
\frac
{\tilde{e}_{i}(\mathbf{\beta}_{0} + \mathbf{\delta} + \Delta\mathbf{\delta}) - \tilde{e}_{i}(\mathbf{\beta}_{0} + \mathbf{\delta})}
{\Delta\mathbf{\delta}}
\right]
\end{align}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\frac{\partial E_{\rho}(\mathbf{\beta})}{\partial \mathbf{\beta}}\Big|_{\mathbf{\beta}_{0} + \mathbf{\delta}} = \mathbf{0}\)</span> とおけば最適なパラメータ <span class="math notranslate nohighlight">\(\mathbf{\beta}_{0} + \mathbf{\delta}^{*}\)</span> を導出することができるだろう。</p>
<p><span class="math notranslate nohighlight">\(\mathbf{r}_{i}\)</span> の微分を <span class="math notranslate nohighlight">\(J_{i}\)</span> とおいて、関数 <span class="math notranslate nohighlight">\(\tilde{e}_{i}\)</span> を近似する。</p>
<div class="math notranslate nohighlight">
\[J_{i}(\mathbf{\beta}_{0})
=
\frac{\partial \mathbf{r}_{i}}{\partial \mathbf{\beta}}\Big|_{\mathbf{\beta}_{0}}
=
\lim_{\Delta\mathbf{\beta} \to \mathbf{0}} \frac{\mathbf{r}_{i}(\mathbf{\beta}_{0} + \Delta\mathbf{\beta}) - \mathbf{r}_{i}(\mathbf{\beta}_{0})}{\Delta\mathbf{\beta}}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\tilde{e}_{i}(\mathbf{\beta}_{0} + \Delta\mathbf{\beta})
&amp;=
\frac{1}{\sigma_{MAD}} \cdot e_{i}(\mathbf{\beta}_{0} + \Delta\mathbf{\beta}) \\
&amp;=
\frac{1}{\sigma_{MAD}} \cdot \mathbf{r}_{i}(\mathbf{\beta}_{0} + \Delta\mathbf{\beta})^{\top} \mathbf{r}_{i}(\mathbf{\beta}_{0} + \Delta\mathbf{\beta}) \\
&amp;\approx
\frac{1}{\sigma_{MAD}} \cdot \left[
    \mathbf{r}_{i}(\mathbf{\beta}_{0}) + J_{i}\Delta\mathbf{\beta}]^{\top} [\mathbf{r}_{i}(\mathbf{\beta}_{0}) + J_{i}\Delta\mathbf{\beta}
\right] \\
&amp;=
\frac{1}{\sigma_{MAD}} \cdot \left[\mathbf{r}_{i}(\mathbf{\beta}_{0})^{\top}\mathbf{r}_{i}(\mathbf{\beta}_{0}) +
2\Delta\mathbf{\beta}^{\top}J_{i}^{\top}\mathbf{r}_{i}(\mathbf{\beta}_{0}) +
\Delta\mathbf{\beta}^{\top}J_{i}^{\top}J_{i}\Delta\mathbf{\beta} \right]
\end{align}\end{split}\]</div>
<p>この結果を利用すると、 <span class="math notranslate nohighlight">\(\tilde{e}_{i}\)</span> の微分を簡易な式で近似することができる。</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
&amp;\tilde{e}_{i}(\mathbf{\beta}_{0} + (\mathbf{\delta} + \Delta\mathbf{\delta})) - \tilde{e}_{i}(\mathbf{\beta}_{0} + \mathbf{\delta}) \\
&amp;\approx
\frac{1}{\sigma_{MAD}} \cdot
\left\{
    [\mathbf{r}_{i}(\mathbf{\beta}_{0})^{\top}\mathbf{r}_{i}(\mathbf{\beta}_{0})
    + 2(\mathbf{\delta} + \Delta \mathbf{\delta})^{\top}J_{i}^{\top}\mathbf{r}_{i}(\mathbf{\beta}_{0})
    + (\mathbf{\delta} + \Delta \mathbf{\delta})^{\top}J_{i}^{\top}J_{i}(\mathbf{\delta} + \Delta \mathbf{\delta})]
    - [\mathbf{r}_{i}(\mathbf{\beta}_{0})^{\top}\mathbf{r}_{i}(\mathbf{\beta}_{0})
    + 2\mathbf{\delta}^{\top}J_{i}^{\top}\mathbf{r}_{i}(\mathbf{\beta}_{0})
    + \mathbf{\delta}^{\top}J_{i}^{\top}J_{i}\mathbf{\delta}]
\right\} \\
&amp;= \frac{1}{\sigma_{MAD}} \cdot \left[ 2\Delta \mathbf{\delta}^{\top}J_{i}^{\top}\mathbf{r}_{i}(\mathbf{\beta}_{0})
+ 2\Delta \mathbf{\delta}^{\top}J_{i}^{\top}J_{i}\mathbf{\delta}
+ \Delta \mathbf{\delta}^{\top}J_{i}^{\top}J_{i}\Delta \mathbf{\delta} \right]
\end{align}\end{split}\]</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\lim_{\Delta\mathbf{\delta} \to \mathbf{0}}
\frac{\tilde{e}_{i}(\mathbf{\beta}_{0} + \mathbf{\delta} + \Delta\mathbf{\delta}) - \tilde{e}_{i}(\mathbf{\beta}_{0} + \mathbf{\delta})}{\Delta\mathbf{\delta}}
&amp;\approx
\frac{1}{\sigma_{MAD}} \cdot
\lim_{\Delta\mathbf{\delta} \to \mathbf{0}}
\frac{
2\Delta \mathbf{\delta}^{\top}J_{i}^{\top}\mathbf{r}_{i}(\mathbf{\beta}_{0})
+ 2\Delta \mathbf{\delta}^{\top}J_{i}^{\top}J_{i}\mathbf{\delta}
+ \Delta \mathbf{\delta}^{\top}J_{i}^{\top}J_{i}\Delta \mathbf{\delta}}{\Delta\mathbf{\delta}}  \\
&amp;=
\frac{1}{\sigma_{MAD}} \cdot
\lim_{\Delta\mathbf{\delta} \to \mathbf{0}}
\left[
2J_{i}^{\top}\mathbf{r}_{i}(\mathbf{\beta}_{0})
+ 2J_{i}^{\top}J_{i}\mathbf{\delta}
+ J_{i}^{\top}J_{i}\Delta \mathbf{\delta}
\right] \\
&amp;=
\frac{2}{\sigma_{MAD}} \cdot (J_{i}^{\top}\mathbf{r}_{i}(\mathbf{\beta}_{0}) + J_{i}^{\top}J_{i}\mathbf{\delta})
\end{align}\end{split}\]</div>
<p>結果として、誤差関数の微分は</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\frac{\partial E_{\rho}(\mathbf{\beta})}{\partial \mathbf{\beta}}\Big|_{\mathbf{\beta}_{0} + \mathbf{\delta}}
&amp;\approx
\frac{2}{\sigma_{MAD}} \cdot
\sum_{i=1}^{n}
\left[
\frac
{\partial \rho}{\partial \tilde{e}_{i}}\Big|_{\tilde{e}_{i}(\mathbf{\beta}_{0} + \mathbf{\delta})}
\cdot
(J_{i}^{\top}\mathbf{r}_{i}(\mathbf{\beta}_{0}) + J_{i}^{\top}J_{i}\mathbf{\delta})
\right]
\end{align}\]</div>
<p>となり、これを <span class="math notranslate nohighlight">\(\mathbf{0}\)</span> とおけば線型方程式が得られる。</p>
<div class="math notranslate nohighlight">
\[\begin{align}
 \sum_{i=1}^{n}
 \frac{\partial \rho}{\partial \tilde{e}_{i}}\Big|_{\tilde{e}_{i}(\mathbf{\beta}_{0} + \mathbf{\delta})}
 \cdot
 J_{i}^{\top}\mathbf{r}_{i}(\mathbf{\beta}_{0})
=
 -\sum_{i=1}^{n}
 \frac{\partial \rho}{\partial \tilde{e}_{i}}\Big|_{\tilde{e}_{i}(\mathbf{\beta}_{0} + \mathbf{\delta})}
 \cdot
 J_{i}^{\top}J_{i}\mathbf{\delta}
 \end{align}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
b &amp;= \sum_{i=1}^{n}
\frac{\partial \rho}{\partial \tilde{e}_{i}}\Big|_{\tilde{e}_{i}(\mathbf{\beta}_{0} + \mathbf{\delta})}
\cdot
J_{i}^{\top}\mathbf{r}_{i}(\mathbf{\beta}_{0}) \\
A &amp;=
-\sum_{i=1}^{n}
\frac{\partial \rho}{\partial \tilde{e}_{i}}\Big|_{\tilde{e}_{i}(\mathbf{\beta}_{0} + \mathbf{\delta})}
\cdot
J_{i}^{\top}J_{i} \\
A\mathbf{\delta} &amp;= b
\end{align}\end{split}\]</div>
<p>この線型方程式を解けば <span class="math notranslate nohighlight">\(\mathbf{\beta}_{0}\)</span> の周辺で <span class="math notranslate nohighlight">\(E_{\rho}(\mathbf{\beta})\)</span> を近似的に最小化させるパラメータ <span class="math notranslate nohighlight">\(\mathbf{\beta}_{0} + \mathbf{\delta}^{*},\, \mathbf{\delta}^{*} = A^{-1}b\)</span> を見つけることができる。</p>
<p>あとは通常のGauss-Newton法と同じように <span class="math notranslate nohighlight">\(\mathbf{\beta}_{m+1} = \mathbf{\beta}_{m} + \mathbf{\delta}\)</span> とし、誤差関数 <span class="math notranslate nohighlight">\(E_{\rho}\)</span> を最小化する操作を誤差またはパラメータの変化が収束するまで繰り返せばよい。</p>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Localization</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">姿勢最適化</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#gauss-newton">重み付きGauss-Newton</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="robust_stddev_estimator.html">外れ値に頑強な標準偏差推定法</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="index.html" title="previous chapter">Welcome to Localization’s documentation!</a></li>
      <li>Next: <a href="robust_stddev_estimator.html" title="next chapter">外れ値に頑強な標準偏差推定法</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022, Takeshi Ishita.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 5.1.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/gauss_newton.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>